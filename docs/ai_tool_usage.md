<!-- Overview -->
This document captures how AI tools were used across various agents in the Finance Voice Assistant project. It includes model prompts, parameters, and the logic flow for how AI was integrated.

<!-- 1. retriever_agent -->
Purpose: Retrieve top-k relevant document chunks based on the user query.

Query Example:

What’s our risk exposure in Asia tech stocks today, and highlight any earnings surprises?
Method:

Used a simple vector similarity search via FAISS / other retriever (e.g., local embedding store).

Returned the top 3–5 most relevant chunks from ingested financial documents.


<!-- 2. lang_agent -->
Purpose: Generate a financial market summary using the LLM (Ollama + Mistral).

Model: mistral via Ollama (running locally)

Temperature: 4 (Note: unusually high for reasoning, may want to lower to ~0.7–1.2 in production)

Prompt Format:

You are a financial analyst. Based on the following documents, answer this question:

{query}

Documents:
{context}
Typical Input Example:

Query: What’s our risk exposure in Asia tech stocks today, and highlight any earnings surprises?

Context: Aggregated chunks from retriever agent, typically covering recent PDD Holdings earnings drop, Nvidia earnings rise, and risk concentration.

Output:
A natural language summary addressing allocation risks and earnings surprises.


<!-- 3. voice_agent -->
a. STT (Speech-to-Text)
Library: Whisper (via faster-whisper or openai-whisper)

Input: Audio file (WAV or MP3)

Output: Raw transcript string

b. TTS (Text-to-Speech)
Library: Coqui TTS or pyttsx3 (depending on performance needs)

Input: Summary text from orchestrator

Output: MP3 audio response stream

<!--  4. orchestrator -->
Purpose: Orchestrates all agents to produce a single market summary.

Workflow:

Calls api_agent for:

Asia Tech Allocation

Earnings surprises

Sends a static or dynamic query to retriever_agent

Aggregates context → sends to lang_agent for summary

Constructs a final output string:

final_response = (
    f"Today, your Asia tech allocation is {asia_pct}% of AUM.\n"
    f"TSMC earnings surprise: {tsmc_surprise}%. Samsung: {samsung_surprise}%.\n"
    f"{summary}"
)

Note: While the output is partially "templated," the actual summary is fully generated by the LLM, making the system dynamic (only the metadata lines are fixed).

<!-- Ollama Setup (Local) -->
Make sure Ollama is installed and running:

ollama run mistral
Accessible at:

http://host.docker.internal:11434